"""
í‰ê°€ ì‹œìŠ¤í…œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
"""
import unittest
from unittest.mock import Mock, patch, MagicMock
import time
from datetime import datetime

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.data_models import ReasoningDataPoint, EvaluationResult
from evaluation.evaluation_system import ReasoningEvaluator
from evaluation.metrics import (
    AccuracyMetric, CategoryAccuracyMetric, DifficultyAccuracyMetric,
    MathAccuracyMetric, BleuMetric, PerformanceMetric, MetricsCalculator,
    MetricsReport, MetricResult
)


class TestReasoningEvaluator(unittest.TestCase):
    """ReasoningEvaluator ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.mock_db_manager = Mock()
        self.mock_collector = Mock()
        self.evaluator = ReasoningEvaluator(self.mock_db_manager, self.mock_collector)

        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        self.sample_data = [
            ReasoningDataPoint(
                id="test_1",
                category="math",
                difficulty="easy",
                question="2 + 2 = ?",
                correct_answer="4",
                source="test"
            ),
            ReasoningDataPoint(
                id="test_2",
                category="logic",
                difficulty="medium",
                question="ì°¸ ë˜ëŠ” ê±°ì§“?",
                correct_answer="ì°¸",
                source="test"
            ),
            ReasoningDataPoint(
                id="test_3",
                category="science",
                difficulty="hard",
                question="ë¬¼ì˜ í™”í•™ì‹ì€?",
                correct_answer="H2O",
                source="test"
            )
        ]

    def test_create_evaluation_set_balanced(self):
        """ê· í˜•ì¡íŒ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # Mock ë°ì´í„° ì„¤ì •
        self.mock_collector.get_data.return_value = self.sample_data

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        eval_set = self.evaluator.create_evaluation_set(
            test_size=3,
            balance_categories=True,
            balance_difficulties=True
        )

        # ê²€ì¦
        self.assertEqual(len(eval_set), 3)
        self.mock_collector.get_data.assert_called()

    def test_create_evaluation_set_simple(self):
        """ë‹¨ìˆœ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # Mock ë°ì´í„° ì„¤ì •
        self.mock_collector.get_data.return_value = self.sample_data

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        eval_set = self.evaluator.create_evaluation_set(
            test_size=2,
            balance_categories=False
        )

        # ê²€ì¦
        self.assertLessEqual(len(eval_set), 2)

    def test_evaluate_model_success(self):
        """ëª¨ë¸ í‰ê°€ ì„±ê³µ í…ŒìŠ¤íŠ¸"""

        # ê°„ë‹¨í•œ ëª¨ë¸ í•¨ìˆ˜
        def simple_model(prompt):
            if "2 + 2" in prompt:
                return "4"
            elif "ì°¸ ë˜ëŠ” ê±°ì§“" in prompt:
                return "ì°¸"
            else:
                return "ëª¨ë¦„"

        # í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="test_model",
            evaluation_set=self.sample_data[:2],  # ì²« ë‘ ë¬¸ì œë§Œ ì‚¬ìš©
            model_function=simple_model,
            save_results=False
        )

        # ê²°ê³¼ ê²€ì¦
        self.assertIn('accuracy', results)
        self.assertIn('total_questions', results)
        self.assertIn('correct_answers', results)
        self.assertEqual(results['total_questions'], 2)
        self.assertEqual(results['correct_answers'], 2)
        self.assertEqual(results['accuracy'], 1.0)

    def test_evaluate_model_with_errors(self):
        """ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í…ŒìŠ¤íŠ¸"""

        # ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ëª¨ë¸ í•¨ìˆ˜
        def error_model(prompt):
            if "2 + 2" in prompt:
                raise Exception("Model error")
            return "ë‹µë³€"

        # í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="error_model",
            evaluation_set=self.sample_data,
            model_function=error_model,
            save_results=False
        )

        # ê²°ê³¼ ê²€ì¦ (ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ì‹œìŠ¤í…œì´ ê³„ì† ì‘ë™í•´ì•¼ í•¨)
        self.assertIn('accuracy', results)
        self.assertGreaterEqual(results['total_questions'], 0)

    def test_check_answer_exact_match(self):
        """ì •í™•í•œ ì¼ì¹˜ ë‹µë³€ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        result = self.evaluator._check_answer("4", "4", "math")
        self.assertTrue(result)

        result = self.evaluator._check_answer("ì°¸", "ì°¸", "logic")
        self.assertTrue(result)

        result = self.evaluator._check_answer("4", "5", "math")
        self.assertFalse(result)

    def test_check_answer_case_insensitive(self):
        """ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë‹µë³€ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        result = self.evaluator._check_answer("TRUE", "true", "logic")
        self.assertTrue(result)

        result = self.evaluator._check_answer("H2O", "h2o", "science")
        self.assertTrue(result)

    def test_check_math_answer(self):
        """ìˆ˜í•™ ë‹µë³€ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        # ì •í™•í•œ ìˆ«ì ì¼ì¹˜
        result = self.evaluator._check_math_answer("3.14", "3.14")
        self.assertTrue(result)

        # ì˜¤ì°¨ ë²”ìœ„ ë‚´ ì¼ì¹˜
        result = self.evaluator._check_math_answer("3.141", "3.14159")
        self.assertTrue(result)

        # ë‹¤ë¥¸ ìˆ«ì
        result = self.evaluator._check_math_answer("2", "3")
        self.assertFalse(result)

        # í…ìŠ¤íŠ¸ ë‹µë³€
        result = self.evaluator._check_math_answer("ë‹µ: 5", "5")
        self.assertTrue(result)

    def test_check_logic_answer(self):
        """ë…¼ë¦¬ ë‹µë³€ í™•ì¸ í…ŒìŠ¤íŠ¸"""
        # ì˜ˆ/ì•„ë‹ˆì˜¤ íŒ¨í„´
        result = self.evaluator._check_logic_answer("ì˜ˆ", "ì°¸")
        self.assertTrue(result)

        result = self.evaluator._check_logic_answer("ì•„ë‹ˆì˜¤", "ê±°ì§“")
        self.assertTrue(result)

        result = self.evaluator._check_logic_answer("yes", "true")
        self.assertTrue(result)

        # ë°˜ëŒ€ ë‹µë³€
        result = self.evaluator._check_logic_answer("ì˜ˆ", "ì•„ë‹ˆì˜¤")
        self.assertFalse(result)

    def test_create_prompt(self):
        """í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # ê°ê´€ì‹ ë¬¸ì œ
        data_point = ReasoningDataPoint(
            id="test",
            category="math",
            difficulty="easy",
            question="2 + 2 = ?",
            correct_answer="4",
            options=["2", "3", "4", "5"]
        )

        prompt = self.evaluator._create_prompt(data_point)
        self.assertIn("2 + 2 = ?", prompt)
        self.assertIn("A)", prompt)
        self.assertIn("B)", prompt)
        self.assertIn("ì„ íƒì§€:", prompt)

        # ì£¼ê´€ì‹ ë¬¸ì œ
        data_point_subjective = ReasoningDataPoint(
            id="test",
            category="math",
            difficulty="easy",
            question="2 + 2 = ?",
            correct_answer="4"
        )

        prompt = self.evaluator._create_prompt(data_point_subjective)
        self.assertIn("2 + 2 = ?", prompt)
        self.assertNotIn("ì„ íƒì§€:", prompt)

    def test_calculate_statistics(self):
        """í†µê³„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        # ìƒ˜í”Œ í‰ê°€ ê²°ê³¼
        results = [
            EvaluationResult(
                id="eval_1",
                data_point_id="test_1",
                model_name="test_model",
                predicted_answer="4",
                is_correct=True,
                execution_time=0.5,
                metadata={"category": "math", "difficulty": "easy"}
            ),
            EvaluationResult(
                id="eval_2",
                data_point_id="test_2",
                model_name="test_model",
                predicted_answer="ì°¸",
                is_correct=True,
                execution_time=0.3,
                metadata={"category": "logic", "difficulty": "medium"}
            ),
            EvaluationResult(
                id="eval_3",
                data_point_id="test_3",
                model_name="test_model",
                predicted_answer="H2O",
                is_correct=False,
                execution_time=0.8,
                metadata={"category": "science", "difficulty": "hard"}
            )
        ]

        # í†µê³„ ê³„ì‚°
        stats = self.evaluator._calculate_statistics(results, total_time=5.0)

        # ê²€ì¦
        self.assertEqual(stats['total_questions'], 3)
        self.assertEqual(stats['correct_answers'], 2)
        self.assertAlmostEqual(stats['accuracy'], 2 / 3)
        self.assertIn('category_accuracy', stats)
        self.assertIn('difficulty_accuracy', stats)
        self.assertEqual(stats['total_evaluation_time'], 5.0)

    def test_save_evaluation_results(self):
        """í‰ê°€ ê²°ê³¼ ì €ì¥ í…ŒìŠ¤íŠ¸"""
        # Mock ì„¤ì •
        self.mock_db_manager.execute_batch_dml.return_value = 2

        # ìƒ˜í”Œ ê²°ê³¼
        results = [
            EvaluationResult(
                id="eval_1",
                data_point_id="test_1",
                model_name="test_model",
                predicted_answer="4",
                is_correct=True,
                execution_time=0.5
            ),
            EvaluationResult(
                id="eval_2",
                data_point_id="test_2",
                model_name="test_model",
                predicted_answer="ì°¸",
                is_correct=False,
                execution_time=0.3
            )
        ]

        # ì €ì¥ ì‹¤í–‰
        success = self.evaluator._save_evaluation_results(results)

        # ê²€ì¦
        self.assertTrue(success)
        self.mock_db_manager.execute_batch_dml.assert_called_once()

    def test_get_model_performance(self):
        """ëª¨ë¸ ì„±ëŠ¥ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # Mock ë°ì´í„° ì„¤ì •
        self.mock_db_manager.execute_query.return_value = [
            (100, 85, 1.25, datetime.now(), datetime.now())
        ]

        # ì„±ëŠ¥ ì¡°íšŒ
        performance = self.evaluator.get_model_performance("test_model")

        # ê²€ì¦
        self.assertIn('model_name', performance)
        self.assertIn('total_evaluations', performance)
        self.assertIn('accuracy', performance)
        self.assertEqual(performance['total_evaluations'], 100)
        self.assertEqual(performance['correct_answers'], 85)
        self.assertAlmostEqual(performance['accuracy'], 0.85)

    def test_get_model_performance_no_data(self):
        """ë°ì´í„°ê°€ ì—†ëŠ” ëª¨ë¸ ì„±ëŠ¥ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # Mock ë°ì´í„° ì„¤ì • (ê²°ê³¼ ì—†ìŒ)
        self.mock_db_manager.execute_query.return_value = [(0, 0, 0, None, None)]

        # ì„±ëŠ¥ ì¡°íšŒ
        performance = self.evaluator.get_model_performance("nonexistent_model")

        # ê²€ì¦
        self.assertIn('message', performance)

    def test_compare_models(self):
        """ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸"""

        # Mock ì„¤ì •
        def mock_get_performance(model_name):
            if model_name == "model_1":
                return {'accuracy': 0.85, 'total_evaluations': 100}
            elif model_name == "model_2":
                return {'accuracy': 0.90, 'total_evaluations': 100}
            else:
                return {'message': 'í‰ê°€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.'}

        # Mock í•¨ìˆ˜ íŒ¨ì¹˜
        with patch.object(self.evaluator, 'get_model_performance', side_effect=mock_get_performance):
            comparison = self.evaluator.compare_models(["model_1", "model_2", "model_3"])

        # ê²€ì¦
        self.assertIn('model_1', comparison)
        self.assertIn('model_2', comparison)
        self.assertIn('model_3', comparison)
        self.assertEqual(comparison['model_1']['accuracy'], 0.85)
        self.assertEqual(comparison['model_2']['accuracy'], 0.90)

    def test_save_evaluation_format(self):
        """í‰ê°€ìš© í¬ë§· ì €ì¥ í…ŒìŠ¤íŠ¸"""
        # Mock ì„¤ì •
        self.mock_collector.get_data.return_value = self.sample_data

        # íŒŒì¼ ì“°ê¸° ëª¨í‚¹
        with patch('builtins.open', create=True) as mock_open:
            with patch('json.dump') as mock_json_dump:
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                count = self.evaluator.save_evaluation_format("test_eval.json", test_size=3)

                # ê²€ì¦
                self.assertGreater(count, 0)
                mock_open.assert_called_once()
                mock_json_dump.assert_called_once()


class TestMetrics(unittest.TestCase):
    """ì§€í‘œ ê³„ì‚° í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.predictions = ["4", "íŒŒë¦¬", "ì˜ˆ", "3.14", "H2O"]
        self.ground_truth = ["4", "íŒŒë¦¬", "ì˜ˆ", "3.14159", "H2O"]
        self.metadata = [
            {"category": "math", "difficulty": "easy"},
            {"category": "common_sense", "difficulty": "easy"},
            {"category": "logic", "difficulty": "medium"},
            {"category": "math", "difficulty": "hard"},
            {"category": "science", "difficulty": "medium"}
        ]

    def test_accuracy_metric(self):
        """ì •í™•ë„ ì§€í‘œ í…ŒìŠ¤íŠ¸"""
        metric = AccuracyMetric()
        result = metric.calculate(self.predictions, self.ground_truth)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "accuracy")
        self.assertAlmostEqual(result.value, 0.8)  # 4/5 ì •ë‹µ
        self.assertIn('correct_count', result.details)
        self.assertEqual(result.details['correct_count'], 4)

    def test_accuracy_metric_case_insensitive(self):
        """ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        predictions = ["TRUE", "False", "YES"]
        ground_truth = ["true", "false", "yes"]

        metric = AccuracyMetric(case_sensitive=False)
        result = metric.calculate(predictions, ground_truth)

        self.assertEqual(result.value, 1.0)

    def test_category_accuracy_metric(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        metric = CategoryAccuracyMetric()
        result = metric.calculate(self.predictions, self.ground_truth, self.metadata)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "category_accuracy")
        self.assertIn('category_accuracies', result.details)

        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„ í™•ì¸
        cat_acc = result.details['category_accuracies']
        self.assertIn('math', cat_acc)
        self.assertIn('common_sense', cat_acc)
        self.assertIn('logic', cat_acc)
        self.assertIn('science', cat_acc)

    def test_difficulty_accuracy_metric(self):
        """ë‚œì´ë„ë³„ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        metric = DifficultyAccuracyMetric()
        result = metric.calculate(self.predictions, self.ground_truth, self.metadata)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "difficulty_accuracy")
        self.assertIn('difficulty_accuracies', result.details)

        # ê° ë‚œì´ë„ë³„ ì •í™•ë„ í™•ì¸
        diff_acc = result.details['difficulty_accuracies']
        self.assertIn('easy', diff_acc)
        self.assertIn('medium', diff_acc)
        self.assertIn('hard', diff_acc)

    def test_math_accuracy_metric(self):
        """ìˆ˜í•™ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        math_predictions = ["3.14", "2", "5.0", "1/2"]
        math_ground_truth = ["3.14159", "2.0", "5", "0.5"]

        metric = MathAccuracyMetric(tolerance=0.01)
        result = metric.calculate(math_predictions, math_ground_truth)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "math_accuracy")
        self.assertEqual(result.value, 1.0)  # ëª¨ë“  ë‹µì´ í—ˆìš© ì˜¤ì°¨ ë‚´

    def test_bleu_metric(self):
        """BLEU ì ìˆ˜ í…ŒìŠ¤íŠ¸"""
        predictions = ["the cat sat on the mat", "hello world"]
        ground_truth = ["the cat is on the mat", "hello world"]

        metric = BleuMetric(n_gram=2)
        result = metric.calculate(predictions, ground_truth)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "bleu")
        self.assertGreaterEqual(result.value, 0.0)
        self.assertLessEqual(result.value, 1.0)

    def test_performance_metric(self):
        """ì„±ëŠ¥ ì§€í‘œ í…ŒìŠ¤íŠ¸"""
        predictions = ["answer1", "answer2", "answer3"]
        ground_truth = ["truth1", "truth2", "truth3"]
        metadata = [
            {"execution_time": 0.5, "memory_usage": 100},
            {"execution_time": 0.3, "memory_usage": 150},
            {"execution_time": 0.8, "memory_usage": 120}
        ]

        metric = PerformanceMetric()
        result = metric.calculate(predictions, ground_truth, metadata)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "performance")
        self.assertIn('avg_execution_time', result.details)
        self.assertIn('total_execution_time', result.details)
        self.assertAlmostEqual(result.details['avg_execution_time'], 0.533, places=2)

    def test_metrics_calculator(self):
        """ì§€í‘œ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸"""
        calculator = MetricsCalculator()

        # ë“±ë¡ëœ ì§€í‘œ í™•ì¸
        metric_names = calculator.get_metric_names()
        self.assertIn('accuracy', metric_names)
        self.assertIn('category_accuracy', metric_names)

        # ëª¨ë“  ì§€í‘œ ê³„ì‚°
        results = calculator.calculate_all(self.predictions, self.ground_truth, self.metadata)

        self.assertIsInstance(results, dict)
        self.assertIn('accuracy', results)
        self.assertIn('category_accuracy', results)

        # ê° ê²°ê³¼ê°€ MetricResult ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ í™•ì¸
        for result in results.values():
            self.assertIsInstance(result, MetricResult)

    def test_custom_metric(self):
        """ì»¤ìŠ¤í…€ ì§€í‘œ í…ŒìŠ¤íŠ¸"""
        calculator = MetricsCalculator()

        # ì»¤ìŠ¤í…€ ì§€í‘œ í•¨ìˆ˜
        def length_difference_metric(predictions, ground_truth, metadata):
            total_diff = sum(abs(len(p) - len(t)) for p, t in zip(predictions, ground_truth))
            return total_diff / len(predictions)

        # ì»¤ìŠ¤í…€ ì§€í‘œ ë“±ë¡
        calculator.register_custom_metric("length_diff", length_difference_metric)

        # ê³„ì‚°
        result = calculator.calculate_single("length_diff", self.predictions, self.ground_truth)

        self.assertIsInstance(result, MetricResult)
        self.assertEqual(result.name, "length_diff")
        self.assertGreaterEqual(result.value, 0.0)

    def test_metrics_report_generation(self):
        """ì§€í‘œ ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        calculator = MetricsCalculator()
        results = calculator.calculate_all(self.predictions, self.ground_truth, self.metadata)

        # ìš”ì•½ ìƒì„±
        summary = MetricsReport.generate_summary(results)

        self.assertIn('timestamp', summary)
        self.assertIn('metrics_count', summary)
        self.assertIn('primary_metrics', summary)
        self.assertIn('detailed_results', summary)

        # ë””ìŠ¤í”Œë ˆì´ í¬ë§· ìƒì„±
        display_text = MetricsReport.format_for_display(results)

        self.assertIsInstance(display_text, str)
        self.assertIn("í‰ê°€ ê²°ê³¼ ìš”ì•½", display_text)
        self.assertIn("ì •í™•ë„", display_text)

    def test_metrics_export(self):
        """ì§€í‘œ ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸"""
        calculator = MetricsCalculator()
        results = calculator.calculate_all(self.predictions, self.ground_truth, self.metadata)

        # JSON ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        with patch('builtins.open', create=True):
            with patch('json.dump') as mock_dump:
                success = MetricsReport.export_to_json(results, "test_metrics.json")
                self.assertTrue(success)
                mock_dump.assert_called_once()

        # CSV ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        with patch('builtins.open', create=True):
            with patch('csv.writer') as mock_writer:
                mock_writer_instance = Mock()
                mock_writer.return_value = mock_writer_instance

                success = MetricsReport.export_to_csv(results, "test_metrics.csv")
                self.assertTrue(success)
                mock_writer_instance.writerow.assert_called()

    def test_metric_validation(self):
        """ì§€í‘œ ì…ë ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        metric = AccuracyMetric()

        # ê¸¸ì´ê°€ ë‹¤ë¥¸ ì…ë ¥
        with self.assertRaises(ValueError):
            metric.calculate(["a", "b"], ["a"])

        # ë¹ˆ ì…ë ¥
        with self.assertRaises(ValueError):
            metric.calculate([], [])

    def test_statistical_functions(self):
        """í†µê³„ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        from evaluation.metrics import calculate_confidence_interval, calculate_statistical_significance

        # ì‹ ë¢°êµ¬ê°„ ê³„ì‚° í…ŒìŠ¤íŠ¸
        values = [0.8, 0.85, 0.9, 0.75, 0.88]
        try:
            interval = calculate_confidence_interval(values)
            self.assertIsInstance(interval, tuple)
            self.assertEqual(len(interval), 2)
            self.assertLessEqual(interval[0], interval[1])
        except ImportError:
            # scipyê°€ ì—†ëŠ” ê²½ìš° íŒ¨ìŠ¤
            pass

        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • í…ŒìŠ¤íŠ¸
        group1 = [0.8, 0.85, 0.9, 0.75, 0.88]
        group2 = [0.7, 0.75, 0.8, 0.65, 0.78]

        try:
            sig_test = calculate_statistical_significance(group1, group2)
            self.assertIn('p_value', sig_test)
            self.assertIn('significant', sig_test)
            self.assertIsInstance(sig_test['significant'], bool)
        except ImportError:
            # scipyê°€ ì—†ëŠ” ê²½ìš° íŒ¨ìŠ¤
            pass


class TestEvaluationIntegration(unittest.TestCase):
    """í‰ê°€ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.mock_db_manager = Mock()
        self.mock_collector = Mock()
        self.evaluator = ReasoningEvaluator(self.mock_db_manager, self.mock_collector)

        # ë³µí•© í…ŒìŠ¤íŠ¸ ë°ì´í„°
        self.complex_data = [
            ReasoningDataPoint(
                id="complex_1",
                category="math",
                difficulty="easy",
                question="15 + 27 = ?",
                correct_answer="42",
                explanation="15ì™€ 27ì„ ë”í•˜ë©´ 42ì…ë‹ˆë‹¤.",
                source="test"
            ),
            ReasoningDataPoint(
                id="complex_2",
                category="logic",
                difficulty="medium",
                question="ëª¨ë“  ìƒˆëŠ” ë‚  ìˆ˜ ìˆë‹¤. ì°¸ìƒˆëŠ” ìƒˆë‹¤. ë”°ë¼ì„œ ì°¸ìƒˆëŠ” ë‚  ìˆ˜ ìˆë‹¤. ì´ ë…¼ë¦¬ê°€ íƒ€ë‹¹í•œê°€?",
                correct_answer="ì˜ˆ",
                explanation="ì˜¬ë°”ë¥¸ ì‚¼ë‹¨ë…¼ë²•ì…ë‹ˆë‹¤.",
                source="test"
            ),
            ReasoningDataPoint(
                id="complex_3",
                category="science",
                difficulty="hard",
                question="ë¹›ì˜ ì†ë„ëŠ” ì´ˆë‹¹ ëª‡ kmì¸ê°€?",
                correct_answer="300000",
                explanation="ì§„ê³µì—ì„œ ë¹›ì˜ ì†ë„ëŠ” ì•½ 30ë§Œ km/sì…ë‹ˆë‹¤.",
                source="test"
            ),
            ReasoningDataPoint(
                id="complex_4",
                category="common_sense",
                difficulty="easy",
                question="ë¹„ê°€ ì˜¬ ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€?",
                correct_answer="ìš°ì‚°",
                options=["ìš°ì‚°", "ì„ ê¸€ë¼ìŠ¤", "ëª¨ì", "ì¥ê°‘"],
                source="test"
            )
        ]

    def test_full_evaluation_pipeline(self):
        """ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

        # ì™„ì „í•œ ëª¨ë¸ í•¨ìˆ˜
        def comprehensive_model(prompt):
            if "15 + 27" in prompt:
                return "42"
            elif "ì‚¼ë‹¨ë…¼ë²•" in prompt or "ë…¼ë¦¬ê°€ íƒ€ë‹¹" in prompt:
                return "ì˜ˆ"
            elif "ë¹›ì˜ ì†ë„" in prompt:
                return "300000 km/s"
            elif "ë¹„ê°€ ì˜¬ ë•Œ" in prompt:
                return "ìš°ì‚°"
            else:
                return "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"

        # Mock ì„¤ì •
        self.mock_collector.get_data.return_value = self.complex_data

        # í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
        eval_set = self.evaluator.create_evaluation_set(test_size=4)
        self.assertEqual(len(eval_set), 4)

        # ëª¨ë¸ í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="comprehensive_model",
            evaluation_set=eval_set,
            model_function=comprehensive_model,
            save_results=False
        )

        # ê²°ê³¼ ê²€ì¦
        self.assertIn('accuracy', results)
        self.assertIn('category_accuracy', results)
        self.assertIn('difficulty_accuracy', results)
        self.assertEqual(results['total_questions'], 4)
        self.assertGreaterEqual(results['accuracy'], 0.75)  # ìµœì†Œ 75% ì •í™•ë„ ê¸°ëŒ€

        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ í™•ì¸
        cat_acc = results['category_accuracy']
        self.assertIn('math', cat_acc)
        self.assertIn('logic', cat_acc)
        self.assertIn('science', cat_acc)
        self.assertIn('common_sense', cat_acc)

    def test_evaluation_with_timeout(self):
        """íƒ€ì„ì•„ì›ƒì„ í¬í•¨í•œ í‰ê°€ í…ŒìŠ¤íŠ¸"""

        # ëŠë¦° ëª¨ë¸ í•¨ìˆ˜
        def slow_model(prompt):
            time.sleep(0.1)  # 0.1ì´ˆ ì§€ì—°
            return "ë‹µë³€"

        # í‰ê°€ ì‹¤í–‰
        start_time = time.time()
        results = self.evaluator.evaluate_model(
            model_name="slow_model",
            evaluation_set=self.complex_data[:2],  # 2ë¬¸ì œë§Œ ì‚¬ìš©
            model_function=slow_model,
            save_results=False
        )
        end_time = time.time()

        # ì‹¤í–‰ ì‹œê°„ í™•ì¸
        total_time = end_time - start_time
        self.assertGreater(total_time, 0.2)  # ìµœì†Œ 0.2ì´ˆ (2ë¬¸ì œ Ã— 0.1ì´ˆ)

        # í‰ê·  ì‹¤í–‰ ì‹œê°„ í™•ì¸
        self.assertGreater(results['average_execution_time'], 0.1)

    def test_evaluation_error_handling(self):
        """í‰ê°€ ì¤‘ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

        # ê°„í—ì ìœ¼ë¡œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ëª¨ë¸
        def unreliable_model(prompt):
            if "ë¹›ì˜ ì†ë„" in prompt:
                raise Exception("Network timeout")
            return "ë‹µë³€"

        # í‰ê°€ ì‹¤í–‰ (ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ë˜ì–´ì•¼ í•¨)
        results = self.evaluator.evaluate_model(
            model_name="unreliable_model",
            evaluation_set=self.complex_data,
            model_function=unreliable_model,
            save_results=False
        )

        # ê²°ê³¼ ê²€ì¦
        self.assertIn('accuracy', results)
        self.assertLess(results['total_questions'], len(self.complex_data))  # ì¼ë¶€ ë¬¸ì œëŠ” ì‹¤íŒ¨

    def test_multilingual_evaluation(self):
        """ë‹¤êµ­ì–´ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        multilingual_data = [
            ReasoningDataPoint(
                id="ko_1",
                category="math",
                difficulty="easy",
                question="ì‚¼ ë”í•˜ê¸° ë‹¤ì„¯ì€?",
                correct_answer="íŒ”",
                source="korean"
            ),
            ReasoningDataPoint(
                id="en_1",
                category="math",
                difficulty="easy",
                question="Three plus five equals?",
                correct_answer="eight",
                source="english"
            )
        ]

        def multilingual_model(prompt):
            if "ì‚¼ ë”í•˜ê¸° ë‹¤ì„¯" in prompt:
                return "íŒ”"
            elif "Three plus five" in prompt:
                return "eight"
            return "unknown"

        # í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="multilingual_model",
            evaluation_set=multilingual_data,
            model_function=multilingual_model,
            save_results=False
        )

        # ê²°ê³¼ ê²€ì¦
        self.assertEqual(results['accuracy'], 1.0)

    def test_batch_evaluation(self):
        """ë°°ì¹˜ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        # ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ í‰ê°€ ì‹œë®¬ë ˆì´ì…˜
        models = {
            "simple_model": lambda p: "ë‹µë³€",
            "smart_model": lambda p: "42" if "15 + 27" in p else "ë‹µë³€",
            "random_model": lambda p: "ëœë¤"
        }

        results_comparison = {}

        for model_name, model_func in models.items():
            results = self.evaluator.evaluate_model(
                model_name=model_name,
                evaluation_set=self.complex_data[:2],
                model_function=model_func,
                save_results=False
            )
            results_comparison[model_name] = results

        # ê²°ê³¼ ë¹„êµ
        self.assertEqual(len(results_comparison), 3)
        self.assertIn('simple_model', results_comparison)
        self.assertIn('smart_model', results_comparison)
        self.assertIn('random_model', results_comparison)

        # smart_modelì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì•¼ í•¨
        smart_acc = results_comparison['smart_model']['accuracy']
        simple_acc = results_comparison['simple_model']['accuracy']
        self.assertGreaterEqual(smart_acc, simple_acc)

    def test_evaluation_with_custom_metrics(self):
        """ì»¤ìŠ¤í…€ ì§€í‘œë¥¼ í¬í•¨í•œ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        from evaluation.metrics import MetricsCalculator

        # ì»¤ìŠ¤í…€ ì§€í‘œ í•¨ìˆ˜
        def korean_specific_accuracy(predictions, ground_truth, metadata):
            """í•œêµ­ì–´ íŠ¹í™” ì •í™•ë„"""
            correct = 0
            for pred, truth, meta in zip(predictions, ground_truth, metadata):
                # í•œêµ­ì–´ ë¬¸ì œë§Œ ê³ ë ¤
                if meta and meta.get('source') == 'korean':
                    if pred.replace(" ", "") == truth.replace(" ", ""):
                        correct += 1
            korean_count = sum(1 for meta in metadata if meta and meta.get('source') == 'korean')
            return correct / korean_count if korean_count > 0 else 0.0

        # ì§€í‘œ ê³„ì‚°ê¸° ì„¤ì •
        calculator = MetricsCalculator()
        calculator.register_custom_metric("korean_accuracy", korean_specific_accuracy)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ê²°ê³¼
        predictions = ["íŒ”", "eight", "ë‹µë³€", "ìš°ì‚°"]
        ground_truth = ["íŒ”", "eight", "ëª¨ë¦„", "ìš°ì‚°"]
        metadata = [
            {"source": "korean", "category": "math"},
            {"source": "english", "category": "math"},
            {"source": "korean", "category": "logic"},
            {"source": "korean", "category": "common_sense"}
        ]

        # ëª¨ë“  ì§€í‘œ ê³„ì‚°
        results = calculator.calculate_all(predictions, ground_truth, metadata)

        # ì»¤ìŠ¤í…€ ì§€í‘œ í™•ì¸
        self.assertIn('korean_accuracy', results)
        korean_result = results['korean_accuracy']
        self.assertGreaterEqual(korean_result.value, 0.0)
        self.assertLessEqual(korean_result.value, 1.0)

    def test_performance_benchmarking(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í…ŒìŠ¤íŠ¸"""

        # ë‹¤ì–‘í•œ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ë“¤
        def fast_model(prompt):
            return "ë¹ ë¥¸ë‹µë³€"

        def slow_model(prompt):
            time.sleep(0.05)  # 50ms ì§€ì—°
            return "ëŠë¦°ë‹µë³€"

        # ì„±ëŠ¥ ì¸¡ì •
        fast_results = self.evaluator.evaluate_model(
            model_name="fast_model",
            evaluation_set=self.complex_data[:3],
            model_function=fast_model,
            save_results=False
        )

        slow_results = self.evaluator.evaluate_model(
            model_name="slow_model",
            evaluation_set=self.complex_data[:3],
            model_function=slow_model,
            save_results=False
        )

        # ì„±ëŠ¥ ë¹„êµ
        fast_time = fast_results['average_execution_time']
        slow_time = slow_results['average_execution_time']

        self.assertLess(fast_time, slow_time)
        self.assertGreater(slow_time, 0.04)  # ìµœì†Œ 40ms ì´ìƒ

    def test_evaluation_result_persistence(self):
        """í‰ê°€ ê²°ê³¼ ì˜ì†ì„± í…ŒìŠ¤íŠ¸"""
        # Mock DB ì„¤ì •
        self.mock_db_manager.execute_batch_dml.return_value = 4

        def test_model(prompt):
            return "í…ŒìŠ¤íŠ¸ë‹µë³€"

        # í‰ê°€ ì‹¤í–‰ (ê²°ê³¼ ì €ì¥ í¬í•¨)
        results = self.evaluator.evaluate_model(
            model_name="persistence_test_model",
            evaluation_set=self.complex_data,
            model_function=test_model,
            save_results=True
        )

        # DB ì €ì¥ í™•ì¸
        self.mock_db_manager.execute_batch_dml.assert_called_once()

        # í˜¸ì¶œëœ SQL íŒŒë¼ë¯¸í„° í™•ì¸
        call_args = self.mock_db_manager.execute_batch_dml.call_args
        self.assertIsNotNone(call_args)

        sql, params_list = call_args[0]
        self.assertIn("INSERT INTO", sql)
        self.assertEqual(len(params_list), 4)  # 4ê°œ ë¬¸ì œì— ëŒ€í•œ íŒŒë¼ë¯¸í„°


class TestEvaluationEdgeCases(unittest.TestCase):
    """í‰ê°€ ì‹œìŠ¤í…œ ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.mock_db_manager = Mock()
        self.mock_collector = Mock()
        self.evaluator = ReasoningEvaluator(self.mock_db_manager, self.mock_collector)

    def test_empty_evaluation_set(self):
        """ë¹ˆ í‰ê°€ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸"""

        def dummy_model(prompt):
            return "ë‹µë³€"

        # ë¹ˆ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€
        results = self.evaluator.evaluate_model(
            model_name="test_model",
            evaluation_set=[],
            model_function=dummy_model,
            save_results=False
        )

        # ê²°ê³¼ í™•ì¸
        self.assertEqual(results['total_questions'], 0)
        self.assertEqual(results['accuracy'], 0.0)

    def test_unicode_handling(self):
        """ìœ ë‹ˆì½”ë“œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        unicode_data = [
            ReasoningDataPoint(
                id="unicode_1",
                category="language",
                difficulty="medium",
                question="ì´ ë¬¸ì¥ì—ì„œ í•œê¸€ì´ ëª‡ ê¸€ìì¸ê°€? 'ì•ˆë…•í•˜ì„¸ìš” ğŸŒŸ'",
                correct_answer="5",
                source="unicode_test"
            ),
            ReasoningDataPoint(
                id="unicode_2",
                category="math",
                difficulty="easy",
                question="Ï€ì˜ ê·¼ì‚¬ê°’ì€?",
                correct_answer="3.14",
                source="unicode_test"
            )
        ]

        def unicode_model(prompt):
            if "í•œê¸€ì´ ëª‡ ê¸€ì" in prompt:
                return "5"
            elif "Ï€ì˜ ê·¼ì‚¬ê°’" in prompt:
                return "3.14"
            return "ëª¨ë¦„"

        # í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="unicode_model",
            evaluation_set=unicode_data,
            model_function=unicode_model,
            save_results=False
        )

        # ê²°ê³¼ í™•ì¸
        self.assertEqual(results['accuracy'], 1.0)

    def test_very_long_inputs(self):
        """ë§¤ìš° ê¸´ ì…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        long_question = "ì´ê²ƒì€ ë§¤ìš° ê¸´ ì§ˆë¬¸ì…ë‹ˆë‹¤. " * 100  # ê¸´ í…ìŠ¤íŠ¸ ìƒì„±

        long_data = [
            ReasoningDataPoint(
                id="long_1",
                category="reading_comprehension",
                difficulty="hard",
                question=long_question + " ì´ ê¸€ì˜ ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€?",
                correct_answer="ë°˜ë³µë˜ëŠ” ë¬¸ì¥ì— ëŒ€í•œ ì´í•´",
                source="long_text_test"
            )
        ]

        def patient_model(prompt):
            if len(prompt) > 1000:
                return "ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ"
            return "ì§§ì€ ë‹µë³€"

        # í‰ê°€ ì‹¤í–‰
        results = self.evaluator.evaluate_model(
            model_name="patient_model",
            evaluation_set=long_data,
            model_function=patient_model,
            save_results=False
        )

        # ê²°ê³¼ í™•ì¸ (ì˜¤ë¥˜ ì—†ì´ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨)
        self.assertEqual(results['total_questions'], 1)

    def test_numerical_precision(self):
        """ìˆ«ì ì •ë°€ë„ í…ŒìŠ¤íŠ¸"""
        precision_data = [
            ReasoningDataPoint(
                id="precision_1",
                category="math",
                difficulty="hard",
                question="Ï€ë¥¼ ì†Œìˆ˜ì  6ìë¦¬ê¹Œì§€ êµ¬í•˜ë©´?",
                correct_answer="3.141593",
                source="precision_test"
            )
        ]

        def precise_model(prompt):
            return "3.141592"  # ì•½ê°„ ë‹¤ë¥¸ ê°’

        # ìˆ˜í•™ ì „ìš© ì§€í‘œë¡œ í‰ê°€
        from evaluation.metrics import MathAccuracyMetric

        math_metric = MathAccuracyMetric(tolerance=0.000001)  # ë§¤ìš° ë‚®ì€ í—ˆìš© ì˜¤ì°¨

        predictions = ["3.141592"]
        ground_truth = ["3.141593"]

        result = math_metric.calculate(predictions, ground_truth)

        # í—ˆìš© ì˜¤ì°¨ë¥¼ ë²—ì–´ë‚˜ë¯€ë¡œ í‹€ë ¤ì•¼ í•¨
        self.assertEqual(result.value, 0.0)

    def test_concurrent_evaluation(self):
        """ë™ì‹œ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        import threading
        import time

        results_list = []
        errors_list = []

        def evaluation_worker(worker_id):
            try:
                def worker_model(prompt):
                    time.sleep(0.01)  # ì§§ì€ ì§€ì—°
                    return f"worker_{worker_id}_response"

                test_data = [
                    ReasoningDataPoint(
                        id=f"concurrent_{worker_id}",
                        category="test",
                        difficulty="easy",
                        question="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
                        correct_answer="í…ŒìŠ¤íŠ¸ ë‹µë³€",
                        source="concurrent_test"
                    )
                ]

                results = self.evaluator.evaluate_model(
                    model_name=f"worker_model_{worker_id}",
                    evaluation_set=test_data,
                    model_function=worker_model,
                    save_results=False
                )

                results_list.append(results)

            except Exception as e:
                errors_list.append(str(e))

        # ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¡œ ë™ì‹œ í‰ê°€
        threads = []
        for i in range(3):
            thread = threading.Thread(target=evaluation_worker, args=(i,))
            threads.append(thread)
            thread.start()

        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()

        # ê²°ê³¼ í™•ì¸
        self.assertEqual(len(errors_list), 0, f"ë™ì‹œ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors_list}")
        self.assertEqual(len(results_list), 3)

        # ê° ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
        for results in results_list:
            self.assertIn('accuracy', results)
            self.assertEqual(results['total_questions'], 1)


if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì„¤ì •
    unittest.main(verbosity=2, buffer=True)